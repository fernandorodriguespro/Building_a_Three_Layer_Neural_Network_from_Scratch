{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Three_Layer_Neural_Network_Commented():\n",
    "    \n",
    "    def __init__(self):     \n",
    "        # self.synaptic_weight_0 = 2*np.random.random((3,4)) - 1 \n",
    "        self.synaptic_weight_0 = np.asarray([[-0.1556352, 0.03320346, -0.13355978, 0.57232395], \n",
    "                                             [-0.09049334, 0.25541687, -0.62995156, -0.03174671], \n",
    "                                             [-0.55489966, 0.43029212, -0.77468673, -0.75519486]])\n",
    "        print(\"self.synaptic_weight_0\")\n",
    "        print(self.synaptic_weight_0)\n",
    "        print()\n",
    "        \n",
    "        # self.synaptic_weight_1 = 2*np.random.random((4,1)) - 1\n",
    "        self.synaptic_weight_1 = np.array([[-0.77467824], \n",
    "                                           [-0.63463163], \n",
    "                                           [ 0.76931812], \n",
    "                                           [ 0.06303952]])\n",
    "        print(\"self.synaptic_weight_1\")\n",
    "        print(self.synaptic_weight_1)\n",
    "        print()\n",
    "        \n",
    "        \n",
    "    # This is a function that \"squishes\"/maps a value to a value between 0 and 1\n",
    "    # Used to convert numbers into probabilities and/or normalise them between 0 and 1 \n",
    "    # This is the equivalent of the step function of a perceptron. \n",
    "    # This activation function returns continuous values instead of discrete like the step funcion of the perceptron\n",
    "    def __sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    # Returns the derivative (slope (inclination, gradient))\n",
    "    # Indicates how confident we are about the existing weight\n",
    "    def __sigmoid_derivative(self, x):\n",
    "        return x * (1 - x)\n",
    "    \n",
    "    # This training process is a trial and error process using random initialized synaptic_weights\n",
    "    # These random initialized synaptic weights are updated every iteration\n",
    "    def train(self, training_set_inputs, training_set_outputs, number_of_training_iterations):\n",
    "\n",
    "        print('layer0 training_set_inputs')\n",
    "        print(training_set_inputs)\n",
    "        print()\n",
    "        \n",
    "        print('training_set_outputs)')\n",
    "        print(training_set_outputs)\n",
    "        print()\n",
    "        \n",
    "        debug_print_frequency = 5000\n",
    "           \n",
    "        for iteration in range(number_of_training_iterations):        \n",
    "            \n",
    "            if iteration % debug_print_frequency == 0:\n",
    "                print()\n",
    "                print(\"##### Iteration: \", iteration)\n",
    "                print()\n",
    "            \n",
    "            # Forward propagate through layers 0, 1, and 2\n",
    "            layer0 = training_set_inputs\n",
    "            \n",
    "            layer1_before_sigmoid = np.dot(layer0,self.synaptic_weight_0)\n",
    "            if iteration % debug_print_frequency == 0:\n",
    "                print(\"layer1_before_sigmoid\")\n",
    "                print(layer1_before_sigmoid)\n",
    "                print()\n",
    "            \n",
    "            layer1 = self.__sigmoid(np.dot(layer0,self.synaptic_weight_0))\n",
    "            if iteration % debug_print_frequency == 0:\n",
    "                print(\"layer1_sigmoid\")\n",
    "                print(layer1)  \n",
    "                print()\n",
    "                \n",
    "            layer2_before_sigmoid = np.dot(layer1,self.synaptic_weight_1)\n",
    "            if iteration % debug_print_frequency == 0:\n",
    "                print(\"layer2_before_sigmoid\")\n",
    "                print(layer2_before_sigmoid)\n",
    "                print()\n",
    "            \n",
    "            layer2 = self.__sigmoid(np.dot(layer1,self.synaptic_weight_1))\n",
    "            if iteration % debug_print_frequency == 0:\n",
    "                print(\"layer2_sigmoid\")\n",
    "                print(layer2)\n",
    "                print()\n",
    "                               \n",
    "            # Calculate error for layer 2\n",
    "            layer2_error = training_set_outputs - layer2\n",
    "            if iteration % debug_print_frequency == 0:\n",
    "                print(\"layer2_error\")\n",
    "                print(layer2_error)\n",
    "                print()\n",
    "                \n",
    "            sigmoid_derivative_layer2 = self.__sigmoid_derivative(layer2)\n",
    "            if iteration % debug_print_frequency == 0:\n",
    "                print(\"sigmoid_derivative_layer2\")\n",
    "                print(sigmoid_derivative_layer2)\n",
    "                print()\n",
    "            \n",
    "            # Use the error to compute the gradient\n",
    "            layer2_gradient = layer2_error * sigmoid_derivative_layer2\n",
    "            if iteration % debug_print_frequency == 0:\n",
    "                print(\"layer2_gradient\")\n",
    "                print(layer2_gradient)\n",
    "                print()\n",
    "                      \n",
    "            synaptic_weight_1_transpose = self.synaptic_weight_1.T   \n",
    "            if iteration % debug_print_frequency == 0:\n",
    "                print(\"synaptic_weight_1_transpose\")\n",
    "                print(synaptic_weight_1_transpose)\n",
    "                print()\n",
    "            \n",
    "            \n",
    "            # Calculate error for layer 1\n",
    "            # This error calculation uses \"confidence weighted error\" from layer2 to establish an error for layer1\n",
    "            # To do this we simply send the error across the weights from layer2 to layer1\n",
    "            # This gives what you could call a \"contribuition weighted error\" because we learn how much each node\n",
    "            # in layer1 \"contributed\" to the error in layer2. This step is called BACKPROPAGATION\n",
    "            # WHY IS THE layer_gradient MATRIX MULTIPLIED BY THE TRANSPOSE OF synaptic_weight_1?\n",
    "                # It is an elegant solution, because it creates an implicit summation through matrix multiplication\n",
    "                # https://gist.github.com/denzilc/1332063\n",
    "            layer1_error = layer2_gradient.dot(self.synaptic_weight_1.T)\n",
    "            if iteration % debug_print_frequency == 0:\n",
    "                print(\"layer1_error\")\n",
    "                print(layer1_error)\n",
    "                print()\n",
    "\n",
    "\n",
    "            sigmoid_derivative_layer1 = self.__sigmoid_derivative(layer2)\n",
    "            if iteration % debug_print_frequency == 0:\n",
    "                print(\"sigmoid_derivative_layer1\")\n",
    "                print(sigmoid_derivative_layer1)\n",
    "                print()\n",
    "                \n",
    "            # Use it to compute its gradient\n",
    "            layer1_gradient = layer1_error * sigmoid_derivative_layer1\n",
    "            if iteration % debug_print_frequency == 0:\n",
    "                print(\"layer1_gradient\")\n",
    "                print(layer1_gradient)\n",
    "                print()\n",
    "                            \n",
    "            # Update the weights using the gradients\n",
    "            layer1_transpose = layer1.T  \n",
    "            \n",
    "            self.synaptic_weight_1 += layer1_transpose.dot(layer2_gradient)\n",
    "            \n",
    "            if iteration % debug_print_frequency == 0:\n",
    "                print(\"layer1_transpose\")\n",
    "                print(layer1_transpose)\n",
    "                print()\n",
    "            \n",
    "            if iteration % debug_print_frequency == 0:\n",
    "                print(\"updated self.synaptic_weight_1\")\n",
    "                print(self.synaptic_weight_1)\n",
    "                print()\n",
    "            \n",
    "            \n",
    "            self.synaptic_weight_0 += layer0.T.dot(layer1_gradient)\n",
    "            if iteration % debug_print_frequency == 0:\n",
    "                print(\"updated self.synaptic_weight_0\")\n",
    "                print(self.synaptic_weight_0)\n",
    "                print()\n",
    "            \n",
    "            layer0_transpose = layer0.T\n",
    "            if iteration % debug_print_frequency == 0:\n",
    "                print(\"layer0_transpose\")\n",
    "                print(layer0_transpose)\n",
    "                print()\n",
    "                \n",
    "            if iteration % debug_print_frequency == 0:\n",
    "                print(\"updated self.synaptic_weight_0\")\n",
    "                print(self.synaptic_weight_0)\n",
    "                print()\n",
    "            \n",
    "            \n",
    "    \n",
    "    def predict(self, inputs):\n",
    "        print('starting Prediction')\n",
    "        print('inputs')\n",
    "        print(inputs)\n",
    "        print()\n",
    "        \n",
    "        layer1 = self.__sigmoid(np.dot(inputs,self.synaptic_weight_0))\n",
    "        print('layer1')\n",
    "        print(layer1)\n",
    "        print()\n",
    "        \n",
    "        layer2 = self.__sigmoid(np.dot(layer1,self.synaptic_weight_1)) \n",
    "        print('layer2')\n",
    "        print(layer2)\n",
    "        print()\n",
    "        \n",
    "        return layer2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.synaptic_weight_0\n",
      "[[-0.1556352   0.03320346 -0.13355978  0.57232395]\n",
      " [-0.09049334  0.25541687 -0.62995156 -0.03174671]\n",
      " [-0.55489966  0.43029212 -0.77468673 -0.75519486]]\n",
      "\n",
      "self.synaptic_weight_1\n",
      "[[-0.77467824]\n",
      " [-0.63463163]\n",
      " [ 0.76931812]\n",
      " [ 0.06303952]]\n",
      "\n",
      "layer0 training_set_inputs\n",
      "[[0 0 1]\n",
      " [1 1 1]\n",
      " [1 0 1]\n",
      " [0 1 1]]\n",
      "\n",
      "training_set_outputs)\n",
      "[[0]\n",
      " [1]\n",
      " [1]\n",
      " [0]]\n",
      "\n",
      "\n",
      "##### Iteration:  0\n",
      "\n",
      "layer1_before_sigmoid\n",
      "[[-0.55489966  0.43029212 -0.77468673 -0.75519486]\n",
      " [-0.8010282   0.71891245 -1.53819807 -0.21461762]\n",
      " [-0.71053486  0.46349558 -0.90824651 -0.18287091]\n",
      " [-0.645393    0.68570899 -1.40463829 -0.78694157]]\n",
      "\n",
      "layer1_sigmoid\n",
      "[[ 0.3647284   0.60594342  0.31546615  0.31969042]\n",
      " [ 0.30980562  0.67236749  0.17679738  0.4465506 ]\n",
      " [ 0.32948067  0.6138431   0.28735879  0.45440925]\n",
      " [ 0.34402846  0.66501169  0.19708112  0.31282575]]\n",
      "\n",
      "layer2_before_sigmoid\n",
      "[[-0.40425106]\n",
      " [-0.50254159]\n",
      " [-0.39508968]\n",
      " [-0.51721036]]\n",
      "\n",
      "layer2_sigmoid\n",
      "[[ 0.40029141]\n",
      " [ 0.37694357]\n",
      " [ 0.40249267]\n",
      " [ 0.37350478]]\n",
      "\n",
      "layer2_error\n",
      "[[-0.40029141]\n",
      " [ 0.62305643]\n",
      " [ 0.59750733]\n",
      " [-0.37350478]]\n",
      "\n",
      "sigmoid_derivative_layer2\n",
      "[[ 0.2400582 ]\n",
      " [ 0.23485712]\n",
      " [ 0.24049232]\n",
      " [ 0.23399896]]\n",
      "\n",
      "layer2_gradient\n",
      "[[-0.09609323]\n",
      " [ 0.14632924]\n",
      " [ 0.14369592]\n",
      " [-0.08739973]]\n",
      "\n",
      "synaptic_weight_1_transpose\n",
      "[[-0.77467824 -0.63463163  0.76931812  0.06303952]]\n",
      "\n",
      "layer1_error\n",
      "[[ 0.07444134  0.06098381 -0.07392627 -0.00605767]\n",
      " [-0.11335807 -0.09286516  0.11257373  0.00922452]\n",
      " [-0.11131811 -0.09119398  0.11054788  0.00905852]\n",
      " [ 0.06770667  0.05546663 -0.06723819 -0.00550964]]\n",
      "\n",
      "sigmoid_derivative_layer1\n",
      "[[ 0.2400582 ]\n",
      " [ 0.23485712]\n",
      " [ 0.24049232]\n",
      " [ 0.23399896]]\n",
      "\n",
      "layer1_gradient\n",
      "[[ 0.01787025  0.01463966 -0.01774661 -0.00145419]\n",
      " [-0.02662295 -0.02181004  0.02643874  0.00216645]\n",
      " [-0.02677115 -0.02193145  0.02658592  0.00217851]\n",
      " [ 0.01584329  0.01297913 -0.01573367 -0.00128925]]\n",
      "\n",
      "layer1_transpose\n",
      "[[ 0.3647284   0.30980562  0.32948067  0.34402846]\n",
      " [ 0.60594342  0.67236749  0.6138431   0.66501169]\n",
      " [ 0.31546615  0.17679738  0.28735879  0.19708112]\n",
      " [ 0.31969042  0.4465506   0.45440925  0.31282575]]\n",
      "\n",
      "updated self.synaptic_weight_1\n",
      "[[-0.74711552]\n",
      " [-0.56438676]\n",
      " [ 0.78894203]\n",
      " [ 0.13561871]]\n",
      "\n",
      "updated self.synaptic_weight_0\n",
      "[[-0.2090293  -0.01053804 -0.08053512  0.5766689 ]\n",
      " [-0.101273    0.24658596 -0.61924649 -0.03086951]\n",
      " [-0.57458022  0.41416942 -0.75514235 -0.75359335]]\n",
      "\n",
      "layer0_transpose\n",
      "[[0 1 1 0]\n",
      " [0 1 0 1]\n",
      " [1 1 1 1]]\n",
      "\n",
      "updated self.synaptic_weight_0\n",
      "[[-0.2090293  -0.01053804 -0.08053512  0.5766689 ]\n",
      " [-0.101273    0.24658596 -0.61924649 -0.03086951]\n",
      " [-0.57458022  0.41416942 -0.75514235 -0.75359335]]\n",
      "\n",
      "\n",
      "##### Iteration:  5000\n",
      "\n",
      "layer1_before_sigmoid\n",
      "[[ 0.11073136  0.95845363 -1.29204082 -1.32605151]\n",
      " [-3.20790723 -1.01543552  0.65436657  1.4396513 ]\n",
      " [-2.95777749 -1.1405609   1.14688471  1.36645969]\n",
      " [-0.13939837  1.08357901 -1.78455896 -1.25285991]]\n",
      "\n",
      "layer1_sigmoid\n",
      "[[ 0.52765459  0.72281209  0.21550758  0.20981324]\n",
      " [ 0.03886924  0.26591746  0.65799378  0.80840065]\n",
      " [ 0.04937021  0.24221739  0.75894144  0.79680756]\n",
      " [ 0.46520673  0.74717068  0.14374111  0.22220547]]\n",
      "\n",
      "layer2_before_sigmoid\n",
      "[[-4.56210137]\n",
      " [ 4.53071038]\n",
      " [ 4.93811351]\n",
      " [-4.58941749]]\n",
      "\n",
      "layer2_sigmoid\n",
      "[[ 0.01033223]\n",
      " [ 0.9893418 ]\n",
      " [ 0.99288291]\n",
      " [ 0.01005661]]\n",
      "\n",
      "layer2_error\n",
      "[[-0.01033223]\n",
      " [ 0.0106582 ]\n",
      " [ 0.00711709]\n",
      " [-0.01005661]]\n",
      "\n",
      "sigmoid_derivative_layer2\n",
      "[[ 0.01022547]\n",
      " [ 0.0105446 ]\n",
      " [ 0.00706644]\n",
      " [ 0.00995548]]\n",
      "\n",
      "layer2_gradient\n",
      "[[ -1.05651917e-04]\n",
      " [  1.12386472e-04]\n",
      " [  5.02924982e-05]\n",
      " [ -1.00118354e-04]]\n",
      "\n",
      "synaptic_weight_1_transpose\n",
      "[[-5.12568237 -4.98526361  3.89476376  4.32072893]]\n",
      "\n",
      "layer1_error\n",
      "[[ 0.00054154  0.0005267  -0.00041149 -0.00045649]\n",
      " [-0.00057606 -0.00056028  0.00043772  0.00048559]\n",
      " [-0.00025778 -0.00025072  0.00019588  0.0002173 ]\n",
      " [ 0.00051317  0.00049912 -0.00038994 -0.00043258]]\n",
      "\n",
      "sigmoid_derivative_layer1\n",
      "[[ 0.01022547]\n",
      " [ 0.0105446 ]\n",
      " [ 0.00706644]\n",
      " [ 0.00995548]]\n",
      "\n",
      "layer1_gradient\n",
      "[[  5.53748386e-06   5.38578375e-06  -4.20767225e-06  -4.66785981e-06]\n",
      " [ -6.07429568e-06  -5.90788953e-06   4.61557018e-06   5.12036899e-06]\n",
      " [ -1.82161050e-06  -1.77170724e-06   1.38415572e-06   1.53553900e-06]\n",
      " [  5.10890022e-06   4.96894121e-06  -3.88201180e-06  -4.30658230e-06]]\n",
      "\n",
      "layer1_transpose\n",
      "[[ 0.52765459  0.03886924  0.04937021  0.46520673]\n",
      " [ 0.72281209  0.26591746  0.24221739  0.74717068]\n",
      " [ 0.21550758  0.65799378  0.75894144  0.14374111]\n",
      " [ 0.20981324  0.80840065  0.79680756  0.22220547]]\n",
      "\n",
      "updated self.synaptic_weight_1\n",
      "[[-5.12577784]\n",
      " [-4.98537272]\n",
      " [ 3.89483872]\n",
      " [ 4.32081544]]\n",
      "\n",
      "updated self.synaptic_weight_0\n",
      "[[-3.06851675 -2.09902221  2.43893152  2.69251786]\n",
      " [-0.2501307   0.12512444 -0.49251741  0.07319242]\n",
      " [ 0.11073412  0.9584563  -1.29204291 -1.32605383]]\n",
      "\n",
      "layer0_transpose\n",
      "[[0 1 1 0]\n",
      " [0 1 0 1]\n",
      " [1 1 1 1]]\n",
      "\n",
      "updated self.synaptic_weight_0\n",
      "[[-3.06851675 -2.09902221  2.43893152  2.69251786]\n",
      " [-0.2501307   0.12512444 -0.49251741  0.07319242]\n",
      " [ 0.11073412  0.9584563  -1.29204291 -1.32605383]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "neural_network = Three_Layer_Neural_Network_Commented()\n",
    "training_set_inputs = np.array([[0, 0, 1], [1, 1, 1], [1, 0, 1], [0, 1, 1]])\n",
    "training_set_outputs = np.array([[0],[1], [1], [0]])\n",
    "neural_network.train(training_set_inputs, training_set_outputs, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Considering new situation [1, 0, 0] -> ?: \n",
      "starting Prediction\n",
      "inputs\n",
      "[1 0 1]\n",
      "layer1\n",
      "[ 0.11263794  0.13664507  0.87299222  0.87921293]\n",
      "layer2\n",
      "[ 0.99595915]\n",
      "[ 0.99595915]\n"
     ]
    }
   ],
   "source": [
    "print(\"Considering new situation [1, 0, 0] -> ?: \")\n",
    "print(neural_network.predict(np.array([1, 0, 1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "\n",
    "https://medium.com/technology-invention-and-more/how-to-build-a-simple-neural-network-in-9-lines-of-python-code-cc8f23647ca1\n",
    "\n",
    "http://iamtrask.github.io/2015/07/12/basic-python-network/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
